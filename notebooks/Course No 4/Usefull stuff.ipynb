{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull stuff for the upcoming project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some basic lib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline preprocessor\n",
    "\n",
    "First seen in *04. Logistic Regression / 07. Exercise - Heart disease diagnosis / 05 Solution - Heart disease.ipynb*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# One-hot encoding\n",
    "onehot_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "# Numerical features\n",
    "other_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'), onehot_columns),\n",
    "    ('other', 'passthrough', other_columns)\n",
    "])\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# k-NN estimator\n",
    "knn_estimator = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()), # Standardize features before k-NN\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get proportion of different values in a np.Series\n",
    "\n",
    "Using the pandas.value_counts() function with *nomralize* parameter set to *True* displays the percentage of each values instead of simply the number of occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[1,1,1,2,2,2,3,4,5,5]\n",
    "\n",
    "# Proportion of features in each class\n",
    "pd.value_counts(y, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a decision tree\n",
    "\n",
    "You can find how to do that in *05. Decision trees and SVMs/02. Decision trees/Learn.ipynb*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "\n",
    "# Export decision tree\n",
    "dot_data = export_graphviz(\n",
    "    dt, out_file=None,\n",
    "    feature_names=encoded_df.drop('survived',axis=1).columns, class_names=['died', 'survived'],\n",
    "    filled=True, rounded=True, proportion=True   \n",
    ")\n",
    "\n",
    "import graphviz\n",
    "\n",
    "# Display decision tree\n",
    "graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make cross tab between two columns of an pandas dataframe\n",
    "\n",
    "You can find how to do that in *05. Decision trees and SVMs/02. Decision trees/Learn.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross tabulation of sex and survived\n",
    "crosstab = pd.crosstab(\n",
    "    index=data_df.sex,\n",
    "    columns=data_df.survived,\n",
    "    normalize='index' # Normalize by sex\n",
    ")\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not forget to scale values !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X = scale(data_df.drop('y', axis=1).values) # Rescale input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some default values for various classifiers \n",
    "> * Logistic regression\n",
    "> * SVM with linear kernel\n",
    "> * *k*-NN\n",
    "> * Decision tree\n",
    "> * Random forest\n",
    "> * SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Logistic regression\n",
    "logreg = LogisticRegression(solver='liblinear', random_state=0)\n",
    "\n",
    "# SVM with linear kernel\n",
    "linear_svc = LinearSVC(random_state=0)\n",
    "\n",
    "# k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "# Decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "\n",
    "# Random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=0)\n",
    "\n",
    "# SVM with RBF kernel\n",
    "rbf_svc = SVC(kernel='rbf', C=10, gamma=1, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a cluster found using k-means\n",
    "\n",
    "The for loop depends on the number of cluster (centroids) you'd like to draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters\n",
    "for cluster in [0, 1, 2]:\n",
    "    # Get points in this cluster\n",
    "    idx = (kmeans.labels_ == cluster)\n",
    "\n",
    "    # Plot points\n",
    "    plt.scatter(\n",
    "        X[idx, 2], # Third column: petal length\n",
    "        X[idx, 3], # Fourth column: petal width\n",
    "        label='cluster {}'.format(cluster)\n",
    "    )\n",
    "\n",
    "    # Plot centroid\n",
    "    centroid = kmeans.cluster_centers_[cluster]\n",
    "    plt.plot(centroid[2], centroid[3], marker='*', color='black', markersize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw a pairplot\n",
    "\n",
    "This kind of diagram is very useful when we'd like to find correlation between N features of a dataset.\n",
    "\n",
    "The following example comes from *06. Clustering and dimensionality reduction / 05. Dimension reduction with PCA*, bases on a wine dataset.\n",
    "\n",
    "The *hue* parameter is used to define the color of the plots based on the value in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(\n",
    "    data_df,\n",
    "    # Variables on the x-axes\n",
    "    x_vars=['alcohol', 'phenols', 'color'],\n",
    "    # Variables on the y-axes\n",
    "    y_vars=['alcohol', 'phenols', 'color'],\n",
    "    # Use a different color for each kind of wine\n",
    "    hue='kind'\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
