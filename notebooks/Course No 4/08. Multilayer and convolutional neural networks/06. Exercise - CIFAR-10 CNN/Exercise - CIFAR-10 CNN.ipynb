{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Load data\n",
    "\n",
    "> **Exercise**: Load the CIFAR-10 data. Normalize the images and split them into train, validation and test sets. Define a `get_batches(X, y, batch_size)` function to generate random X/y batches of size `batch_size` using a Python generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and normalize data\n",
    "\n",
    "Normalization is done the same way we've done in previous unit (minus 128 and divide by 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 data shape: (60000, 3072)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "CIFAR_NUMBER=60\n",
    "IMG_WIDTH=32\n",
    "IMG_HEIGHT=32\n",
    "IMG_NBCOLOR=3\n",
    "\n",
    "# Load data\n",
    "with np.load(os.path.join('data', 'cifar10-{}k.npz'.format(CIFAR_NUMBER)), allow_pickle=False) as npz_file:\n",
    "    cifar = dict(npz_file.items())\n",
    "    \n",
    "# Convert pixels into floating point numbers\n",
    "data = cifar['data'].astype(np.float32)\n",
    "\n",
    "# Rescale pixel values between -0.5 and 0.5\n",
    "data = (data - 128) / 255\n",
    "print(\"CIFAR10 data shape:\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (58000, 32, 32, 3) (58000,)\n",
      "Valid: (1000, 32, 32, 3) (1000,)\n",
      "Test: (1000, 32, 32, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    # Reshape images: 32 by 32 with 3 (RGB) color channels\n",
    "    data.reshape(-1, 32, 32, 3),\n",
    "    cifar['labels'],\n",
    "    test_size=2000, random_state=0)\n",
    "\n",
    "# Create validation and test sets\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=1000, random_state=0)\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batch generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    \n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Create and train a ConvNet\n",
    "\n",
    "> **Exercise:** Create a convolutional neural network and train it using your batch generator. Evaluate the accuracy on the validation set after each epoch. Test different architectures and parameters. Evaluate your best network on the test set. Save the trained kernel weights of the first convolutional layer in a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (?, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, IMG_WIDTH, IMG_HEIGHT, IMG_NBCOLOR])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "    \n",
    "    print(\"Input:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build first convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer 1: (?, 16, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Convolution\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        X, # Input data\n",
    "        filters=64, # 16 filters\n",
    "        kernel_size=(5, 5), # Kernel size: 5x5\n",
    "        strides=(2, 2), # Stride: 2\n",
    "        padding='SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(\n",
    "            stddev=0.01, seed=0), # Small standard deviation\n",
    "        name='conv1' # Add name\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the kernel weights\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    print(\"Convolutional layer 1:\", conv1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build first max pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pool 1: (?, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Max pooling layer\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        conv1, # Convolution output\n",
    "        pool_size=(2, 2), # Pool size: 2\n",
    "        strides=(2, 2), # Stride: 2\n",
    "        padding='SAME', # \"same\" padding\n",
    "    )\n",
    "    print(\"Max pool 1:\", pool1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build second convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convultional layer 2: (?, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Convolution\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool1, # Max pooling output\n",
    "        filters=64, # 16 filters\n",
    "        kernel_size=(3, 3), # Kernel size: 3x3\n",
    "        strides=(1, 1), # Stride: 1\n",
    "        padding='SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(\n",
    "            stddev=0.01, seed=0), # Small standard deviation\n",
    "        name='conv2' # Add name\n",
    "    )\n",
    "    \n",
    "    # Get the kernel weights\n",
    "    with tf.variable_scope('conv2', reuse=True):\n",
    "        conv2_kernels = tf.get_variable('kernel')\n",
    "    \n",
    "    print(\"Convultional layer 2:\", conv2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build second max pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pool 2: (?, 4, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    # Max pooling layer (2x2, stride: 2)\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        conv2, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "    \n",
    "    print(\"Max pool 2:\", pool2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten output: (?, 1024)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():    \n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool2)\n",
    "    \n",
    "    print(\"Flatten output:\", flat_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multilayer neural network\n",
    "\n",
    "Based on the flatten output build previously, let's build a *hidden* and *logits* layers.\n",
    "\n",
    "Note that the flatten output layer will be reduced using a 0.5 dropout on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout: (?, 1024)\n",
      "Fully connected layer: (?, 256)\n",
      "Output layer: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    # Create training placeholder\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # dropout of the flat_output\n",
    "    flat_output = tf.layers.dropout(\n",
    "        flat_output, rate=0.5, seed=0, training=training)\n",
    "    print(\"Dropout:\", flat_output.shape)\n",
    "\n",
    "    # Hidden layer with 256 units\n",
    "    hidden = tf.layers.dense(\n",
    "        flat_output, 256, activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='hidden'\n",
    "    )\n",
    "    print(\"Fully connected layer:\", hidden.shape)\n",
    "\n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        hidden, 10, activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='output'\n",
    "    )\n",
    "    \n",
    "    print(\"Output layer:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define regression functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "\n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits))\n",
    "    \n",
    "    # Adam optimizer\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    # gd = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(mean_ce)\n",
    "    \n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the accuracy on the validation set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - valid: 0.526 train: 0.436 (mean)\n",
      "Epoch 2 - valid: 0.647 train: 0.557 (mean)\n",
      "Epoch 3 - valid: 0.687 train: 0.609 (mean)\n",
      "Epoch 4 - valid: 0.708 train: 0.646 (mean)\n"
     ]
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Train several epochs\n",
    "    for epoch in range(15):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "\n",
    "        # Get batches of data\n",
    "        for X_batch, y_batch in get_batches(X_train, y_train, 64):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.001, # Learning rate\n",
    "                training: True # Apply dropout\n",
    "            })\n",
    "\n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid,\n",
    "            y: y_valid,\n",
    "            training: False # Do not apply dropout\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "\n",
    "        # Print progress\n",
    "        print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "            epoch+1, valid_acc, np.mean(batch_acc)\n",
    "        ))\n",
    "\n",
    "    # Weights of the hidden layer\n",
    "    # weights_hidden = W1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Visualize kernels\n",
    "\n",
    "> **Exercise**: Plot the kernels from the first convolutional layer with the `imshow()` function.\n",
    "\n",
    "**Hint**: Remember that the `imshow()` function expects values between 0 and 1 for 3-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
