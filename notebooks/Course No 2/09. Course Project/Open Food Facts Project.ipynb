{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Food Facts Course Project - Cleaning, manipulating and visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import usefull libraries for the project and make matplolib displaying graphs inline the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path # For filepath manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be to load the Open Food Facts CSV file, which in fact is a TSV file (Cells are separated with tabs).\n",
    "Loading this file will take a consequent amount of time, as the file is 1Gb big. First thing to do will be to perform some cleanup and removal of useless data, and save the result as a new CSV file.\n",
    "\n",
    "This new CSV file will be used as datasource in this Notebook using the first **MAX_ENTRY_TO_LOAD** rows while coding.\n",
    "\n",
    "**Don't forget to set MAX_ENTRY_TO_LOAD = None when coding is finished.** Otherwise, only a subset of the data will be processed.\n",
    "\n",
    "* Note that this Notebook checks if the cleaned Datafile exists and create it otherwise. This process relies on three functions: First one will load the original TSV file, the second one will cleanup the orignal data and the third one will dump the cleand data into a new CSV file.\n",
    "\n",
    "The following global constant can be adapted to suit your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename of the original TSV file\n",
    "ORIGINAL_TSV_FILENAME = path.join('data','OpenFoodFacts.tsv')\n",
    "\n",
    "# Filename of the cleaned data build in this Notebook\n",
    "CLEANED_CSV_FILENAME = path.join('data','OpenFoodFacts-cleaned.csv')\n",
    "\n",
    "# If set to true, the original data file loading process is forced, event if the\n",
    "# cleaned CSV file exists. Should be set to **True** when coding is finished\n",
    "FORCE_LOAD_ORIGINAL_FILE = False # PRoduction value = True\n",
    "\n",
    "# Maximum NaN percentage accepted in a column. If above, the column is dropped.\n",
    "MAX_NAN_PERCENT_VALUE = 80\n",
    "\n",
    "# Number of rows loaded from cleaned CSV file. Usefull while coding, this value should be\n",
    "# set ton **None** when coding is finished.\n",
    "MAX_ENTRY_TO_LOAD = None # Production value = None\n",
    "\n",
    "# List of columns that will be removed from the Dataset (useless one)\n",
    "COLUMNS_TO_DROP = [\n",
    "    'creator', 'brands', 'brands_tags', 'categories','main_category', 'countries',\n",
    "    'countries_tags', 'additives', 'additives_tags', 'categories_tags', 'states',\n",
    "    'states_en', 'states_tags', 'url', 'quantity', 'packaging_tags', 'packaging',\n",
    "    'created_t', 'last_modified_t', 'pnns_groups_1', 'pnns_groups_2', 'image_url',\n",
    "    'image_small_url', 'code', 'ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A. Importing and cleaning the data\n",
    "\n",
    "Importing the datasource is done using Pandas **read_csv** method, using parameter **sep=\"\\t\"** as the content of the file is a tabulation spearated CSV file.\n",
    "\n",
    "* Note that I've set the **low_memory** option to False in order to avoid warnings when loading the file. Number of columns is quite important and the process to determine the column dtype is too consuming*\n",
    "\n",
    "\n",
    "### Some function definitions\n",
    "\n",
    "#### Data loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOriginalTsvFile(filename):\n",
    "    print(\"Loading data from file\",ORIGINAL_TSV_FILENAME)\n",
    "    print(\"Please wait...\")\n",
    "    df = pd.read_csv(ORIGINAL_TSV_FILENAME,sep=\"\\t\",low_memory=False)\n",
    "    print(\"Loading process terminated.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to dump the cleaned data into a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpCleanedCsvFile(df,filename):\n",
    "    print(\"Dumping the cleaned Dataframe into file\",CLEANED_CSV_FILENAME)\n",
    "    print(\"Please wait...\")\n",
    "    df.to_csv(CLEANED_CSV_FILENAME)\n",
    "    print(\"Dumping process terminated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup function that process the Dataframe returned by the **loadOriginalTsvFile()** function\n",
    "\n",
    "This function will perform cleanup actions on the whole dataset. Further in this Notebook, more cleaning actions will come while we discover the content of the Open Food Facts database.\n",
    "\n",
    "Here is a list of the cleaning actions done here:\n",
    "\n",
    "* Drop unused column defined in the global parameter COLUMNS_TO_DROP\n",
    "* Drop columns where the percentage of null values is above MAX_NAN_PERCENT_VALUE\n",
    "* Drop rows where **product_name** or **countries_en** column are empty\n",
    "* Drop rows with duplicates in 'product_name' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanOriginalData(df):\n",
    "\n",
    "    print(\"Cleaning the dataframe\")\n",
    "    print(\"Please wait...\")\n",
    "    # Drop unused columns\n",
    "    df.drop(COLUMNS_TO_DROP,axis = 1,inplace=True)\n",
    "    \n",
    "    # Drop columns where percentage of NaN values is too high\n",
    "    df = df.dropna(axis=1, thresh= len(df)*(1 - MAX_NAN_PERCENT_VALUE / 100), how='all') \n",
    "    \n",
    "    # Drop rows with empty product_name or countries_en\n",
    "    df = df[np.logical_and(\n",
    "        np.logical_not(df['product_name'].isnull()),\n",
    "        np.logical_not(df['countries_en'].isnull())\n",
    "    )]\n",
    "    \n",
    "    # Drop duplicated rows in column product_name\n",
    "    df.drop_duplicates(subset=['product_name'],inplace=True)\n",
    "\n",
    "    print(\"Cleaning process terminated\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading process\n",
    "\n",
    "Now that our loading functions are defined, put some logic here to avoid long time processing while coding.\n",
    "\n",
    "**Do not forget to set the global constant to Production values when coding is finished**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV file found. Original data file processing is skipped\n",
      "Loading data from  data/OpenFoodFacts-cleaned.csv\n",
      "Please wait...\n",
      "Dataframe loaded\n",
      "Number of rows   : 249065\n",
      "Number of columns: 28\n"
     ]
    }
   ],
   "source": [
    "if (FORCE_LOAD_ORIGINAL_FILE == True) or path.exists(CLEANED_CSV_FILENAME) == False:\n",
    "    df = loadOriginalTsvFile(ORIGINAL_TSV_FILENAME)\n",
    "    df = cleanOriginalData(df)\n",
    "    dumpCleanedCsvFile(df, CLEANED_CSV_FILENAME)\n",
    "else:\n",
    "    print(\"Cleaned CSV file found. Original data file processing is skipped\")\n",
    "\n",
    "    \n",
    "if MAX_ENTRY_TO_LOAD != None:\n",
    "    print(\"Loading the first\",MAX_ENTRY_TO_LOAD,\"rows from \",CLEANED_CSV_FILENAME)\n",
    "else:\n",
    "    print(\"Loading data from \",CLEANED_CSV_FILENAME)\n",
    "\n",
    "print(\"Please wait...\")\n",
    "df = pd.read_csv(CLEANED_CSV_FILENAME,low_memory=False, nrows=MAX_ENTRY_TO_LOAD, index_col=0)\n",
    "print(\"Dataframe loaded\")\n",
    "\n",
    "print('Number of rows   :',format(df.shape[0]))\n",
    "print('Number of columns:',format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning process\n",
    "\n",
    "Now that we've loaded a *partially* cleaned dataframe, let's explore it and perform some more clean up.\n",
    "\n",
    "First, we can ensure that we do not have any duplicated lines. Removed them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicated lines found\n"
     ]
    }
   ],
   "source": [
    "duplicated_lines = df.duplicated().sum()\n",
    "\n",
    "if duplicated_lines == 0:\n",
    "    print(\"No duplicated lines found\")\n",
    "else:\n",
    "    print(\"Duplicated lines found:\",duplicated_lines)\n",
    "    print(\"Removing duplicates...\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that we do not have duplicated values in the **product_name** columns and set it as Dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no duplicated values in 'product_name' column. Set it as Dataframe index\n"
     ]
    }
   ],
   "source": [
    "if 'product_name' in df.columns:\n",
    "    if df['product_name'].duplicated().sum() == 0:\n",
    "        print(\"There is no duplicated values in 'product_name' column. Set it as Dataframe index\")\n",
    "        df.set_index('product_name', inplace=True)\n",
    "    else:\n",
    "        print(\"WARNING: Duplicated values detected in 'product_name' column. Aborting Notebook.\")\n",
    "        exit(1)\n",
    "else:\n",
    "    print(\"Dataframe already indexed by 'product_name'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date and time columns into DateTime Pandas object, for smarter time manipulations.\n",
    "\n",
    "During the first **to_datetime()** run, I've found values that was not convertible to a Datetime object. For example, one of the row contains value **Dia,Sogeres** in its **created_datetime** cell. Solution is to set the *unparsable* datetime strings to NaT using **error=coerce** paramter in **to_datetime()** call, and then fill NaT value with **fillna()** function using **ffill** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_datetime'] = pd.to_datetime(df['created_datetime'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\n",
    "df['created_datetime'].fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "df['last_modified_datetime'] = pd.to_datetime(df['last_modified_datetime'], format='%Y-%m-%dT%H:%M:%SZ', errors='coerce')\n",
    "df['last_modified_datetime'].fillna(method = 'ffill', inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse countries_en entries to get *usable* data\n",
    "\n",
    "**countries_en** column is a very special one as it contains the different countries where the product is produced as a comma separated string values. Not very usable for data analysis. Let's perform some operations to make it more usable.\n",
    "\n",
    "Consequences, this column contains 642 unique entries, which is far much more than the number of countries existing in the world :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique entries in 'countries_en' 642\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique entries in 'countries_en'\",len(df.countries_en.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what can we do ?\n",
    "\n",
    "The approach I've choose is to *split* comma separated strings into arrays, using the **str.split()** function, and then, for every arrays, I do apply the **Panda.Series** method on it to convert them into **Series**, which by consequence will give me a Dataframe with columns containing the splitted strings.  \n",
    "\n",
    "To do so, I do run the **str.split()** function on the df['countries_en'] serie to get a serie of arrays, use the **apply()** method to run **pd.Series** on each array, and finally run the **stack()** method to transform the serie of arrays into a Dataframe.\n",
    "\n",
    "* Note: The stack method introduces a two level index. Using the **Dataframe.droplevel()** method on the second index (indice = 1) will duplicate entries in the first index and provide a single level index Dataframe. This is exactly what I'm looking for.\n",
    "\n",
    "Looking forward into the Open Food Facts data, we can identify some other columns containing comma separated strings like 'countries_en' ('ingredient_text' for example). For that reason, I've decided to write a generic function that implement this String Splitting algorythm and returns an *expanded* Series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def splitStringSeriesToDataframe(data, column_name, sep = ','):\n",
    "    # Split strings using sep as separator\n",
    "    temp = data[column_name].str.split(sep)\n",
    "    \n",
    "    # apply pd.Series constructor on arrays. This will return a Dataframe\n",
    "    temp = temp.apply(pd.Series,1)\n",
    "    \n",
    "    # stack Dataframe\n",
    "    temp = temp.stack()\n",
    "    \n",
    "    # Drop second level index created by the previous stack() call\n",
    "    temp = temp.droplevel(1) # Second index = indice[1]\n",
    "    \n",
    "    # Rename the index\n",
    "    temp.name = column_name + '_splitted'\n",
    "    \n",
    "    return temp\n",
    "\n",
    "\n",
    "countries = splitStringSeriesToDataframe(df,'countries_en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr:Angleterre\n",
      "Angleterre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "product_name\n",
       "A&W racinette/root beer                                                      fr:Quebec\n",
       "Natur-a Soya Chocolat                                                        fr:Quebec\n",
       "Spaghettini                                                                  fr:Quebec\n",
       "Mapple Syrup - Sirop d'erable                                                fr:Quebec\n",
       "Bouillon de poulet Loney’s                                                   fr:Quebec\n",
       "eau gazeuse perrier                                                          fr:Quebec\n",
       "Gâteries à l'érable du Canada                                                fr:Quebec\n",
       "Sirop d'érable pur biologique Escuminac                                      fr:Quebec\n",
       "Cake Tiramisu                                                            fr:Angleterre\n",
       "Cake Tiramisu                                                       fr:Grande-bretagne\n",
       "Coca cola                                                                    fr:Quebec\n",
       "Organic udon noodles                                                    fr:Deutschland\n",
       "Fondant d'érable biologique                                                  fr:Quebec\n",
       "Ananas Stücke leicht gezuckert                                          fr:Deutschland\n",
       "Pains pour Hamburger x6                                                 fr:Deutschland\n",
       "Pains pour Hamburger x6                                                  fr:Frankreich\n",
       "Blanc de Poulet Rôti                                                    fr:Deutschland\n",
       "Céréales fourrées à la noisette                                         fr:Deutschland\n",
       "Céréales fourrées à la noisette                                          fr:Frankreich\n",
       "Pizza Kombi                                                             fr:Deutschland\n",
       "Bio Confiture Extra Fraise                                              fr:Deutschland\n",
       "Bio Confiture Extra Fraise                                               fr:Frankreich\n",
       "Mango Scheiben gezuckert                                                fr:Deutschland\n",
       "Galettes de riz bio chocolat noir                                       fr:Deutschland\n",
       "Galettes de riz bio chocolat noir                                           fr:Spanien\n",
       "Bio Mozzarella                                                          fr:Deutschland\n",
       "Vegetarisches Gemüseschnitzel                                           fr:Deutschland\n",
       "Vegetarische Burgerscheiben auf Basis von Soja- und Weizeneiweiß        fr:Deutschland\n",
       "Vegetarische Burgerscheiben auf Basis von Soja- und Weizeneiweiß            fr:Spanien\n",
       "Italiamo Sugo Alla Toscana, Tomatensoße Mit Geräuc...                   fr:Deutschland\n",
       "                                                                           ...        \n",
       "Bäckers Vital+fit - Harry - 500 G                                       fr:Deutschland\n",
       "Bäckers Vital+fit - Harry - 500 G                                           fr:Schweiz\n",
       "Fettarmer Bio-Kefir mild                                                 fr:Frankreich\n",
       "yogourt                                                                      fr:Quebec\n",
       "fécule                                                                       fr:Quebec\n",
       "farine amaranthe                                                             fr:Quebec\n",
       "mélange à gâteau                                                             fr:Quebec\n",
       "Fritz-limo Zitrone                                                      fr:Deutschland\n",
       "Smoothie, Detox Orange, with Mango, Acerola et Kurkuma                  fr:Deutschland\n",
       "BeerenMuesli                                                            fr:Deutschland\n",
       "Nu3 Peanut Butter                                                       fr:Deutschland\n",
       "Snickers White                                                          fr:Deutschland\n",
       "Kitkat Mini                                                               fr:Nederland\n",
       "Uncle Ben's - 250G                                                      fr:Deutschland\n",
       "Uncle Ben's - 250G                                                           fr:Irland\n",
       "Uncle Ben's - 250G                                                          fr:Schweiz\n",
       "Innocent coconut water 1l                                               fr:Deutschland\n",
       "Coco Original                                                               nl:Belgien\n",
       "Coco Original                                                           nl:Deutschland\n",
       "Coco Original                                                            nl:Frankreich\n",
       "Coco Original                                                               nl:Schweiz\n",
       "Thon Listao                                                                  fr:Quebec\n",
       "vh                                                                           fr:Quebec\n",
       "Clover Leaf                                                                  fr:Quebec\n",
       "mangue                                                                       fr:Quebec\n",
       "sucanat                                                                      fr:Quebec\n",
       "becel margarine                                                              fr:Quebec\n",
       "Wasa - 290G                                                             fr:Deutschland\n",
       "Spaghetti alla Carbonara                                                fr:Deutschland\n",
       "Herbal infusion Mint                                                      fr:Nederland\n",
       "Name: countries_en_splitted, Length: 91, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in countries.unique():\n",
    "    if 'Angleterre' in i:\n",
    "        print(i)\n",
    "        \n",
    "countries[countries.str.contains(':')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
