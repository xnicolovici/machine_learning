{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning hyperparameters with grid search\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_df = pd.read_csv('house-prices.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df.hist(column='SalePrice', bins=20, grid=False, xrot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.hist(np.log10(data_df.SalePrice), bins=20)\n",
    "plt.xlabel('log10(SalePrice)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Work on a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # One-hot encoding\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    \n",
    "    # Fill missing values\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "        \n",
    "    return df\n",
    "\n",
    "preprocessed_df = preprocess(data_df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create X, y\n",
    "X = preprocessed_df.drop('SalePrice', axis=1).values\n",
    "y = np.log10(preprocessed_df.SalePrice).values\n",
    "\n",
    "# Split into train/test sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "print('Train:', X_tr.shape, y_tr.shape)\n",
    "print('Test:', X_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_tr_rescaled = scaler.fit_transform(X_tr)\n",
    "X_te_rescaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Try with a linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr_rescaled, y_tr)\n",
    "\n",
    "print('Train MSE: {:.4f}'.format(MSE(y_tr, lr.predict(X_tr_rescaled))))\n",
    "print('Test MSE: {:.4f}'.format(MSE(y_te, lr.predict(X_te_rescaled))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "# Variable to store the results\n",
    "gs_results = []\n",
    "\n",
    "# Grid search\n",
    "for alpha in np.logspace(-10, 10, num=100):\n",
    "    # Create and fit ridge regression\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_tr_rescaled, y_tr)\n",
    "    \n",
    "    # Save model and its performance on train/test sets\n",
    "    gs_results.append({\n",
    "        'alpha': alpha,\n",
    "        'train_mse': MSE(y_tr, ridge.predict(X_tr_rescaled)),\n",
    "        'train_mae': MAE(10**y_tr, 10**ridge.predict(X_tr_rescaled)),\n",
    "        'test_mse': MSE(y_te, ridge.predict(X_te_rescaled)),\n",
    "        'test_mae': MAE(10**y_te, 10**ridge.predict(X_te_rescaled)),\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "gs_results = pd.DataFrame(gs_results)\n",
    "gs_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation curves\n",
    "plt.semilogx(gs_results['alpha'], gs_results['train_mse'], label='train curve')\n",
    "plt.semilogx(gs_results['alpha'], gs_results['test_mse'], label='test curve')\n",
    "plt.xlabel('$log_{10}(alpha)$')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model with a very strong regularization\n",
    "strong_ridge = Ridge(alpha=10**10)\n",
    "strong_ridge.fit(X_tr_rescaled, y_tr)\n",
    "\n",
    "print('Largest coefficient: {:.1e}'.format(\n",
    "    np.max(np.abs(strong_ridge.coef_))))\n",
    "print('Intercept: {:.2f}'.format(\n",
    "    strong_ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean target value: {:.2f}'.format(np.mean(y_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entry with the best test MSE\n",
    "best_result = gs_results.loc[gs_results.test_mse.idxmin()]\n",
    "\n",
    "# Print the details\n",
    "print('Best alpha: {:.1e}'.format(best_result.alpha))\n",
    "print('Test MSE: {:.4f}'.format(best_result.test_mse))\n",
    "print('Test MAE: {:,.0f}$'.format(best_result.test_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/test N models\n",
    "gs_results = []\n",
    "for run_idx in range(10):\n",
    "    # Split into train/test sets\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=run_idx)\n",
    "    \n",
    "    # Standardize features\n",
    "    X_tr_rescaled = scaler.fit_transform(X_tr)\n",
    "    X_te_rescaled = scaler.transform(X_te)\n",
    "\n",
    "    # Grid search\n",
    "    for alpha in np.logspace(1, 4, num=20):\n",
    "        # Create and fit ridge regression\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X_tr_rescaled, y_tr)\n",
    "\n",
    "        # Save model and its performance on train/test sets\n",
    "        gs_results.append({\n",
    "            'alpha': alpha,\n",
    "            'run_idx': run_idx,\n",
    "            'train_mse': MSE(y_tr, ridge.predict(X_tr_rescaled)),\n",
    "            'train_mae': MAE(10**y_tr, 10**ridge.predict(X_tr_rescaled)),\n",
    "            'test_mse': MSE(y_te, ridge.predict(X_te_rescaled)),\n",
    "            'test_mae': MAE(10**y_te, 10**ridge.predict(X_te_rescaled)),\n",
    "        })\n",
    "        \n",
    "# Convert results to DataFrame\n",
    "gs_results = pd.DataFrame(gs_results)\n",
    "gs_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results by alpha value\n",
    "gb_alpha = gs_results.groupby('alpha')\n",
    "\n",
    "# Compute train/test mean scores with std\n",
    "mean_tr = gb_alpha.train_mse.mean()\n",
    "mean_te = gb_alpha.test_mse.mean()\n",
    "std_tr = gb_alpha.train_mse.std()\n",
    "std_te = gb_alpha.test_mse.std()\n",
    "alphas = mean_tr.index.values\n",
    "\n",
    "# Get entry with the best mean test MSE\n",
    "best_alpha = mean_te.idxmin()\n",
    "best_result = gb_alpha.get_group(best_alpha)\n",
    "\n",
    "# Print the details\n",
    "print('Best alpha: {:.1e}'.format(best_alpha))\n",
    "print('Test MSE: {:.4f}'.format(best_result.test_mse.mean()))\n",
    "print('Test MAE: {:,.0f}$'.format(best_result.test_mae.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean scores\n",
    "plt.plot(np.log10(alphas), mean_tr, label='train')\n",
    "plt.plot(np.log10(alphas), mean_te, label='test')\n",
    "\n",
    "# Quantify variance with Â±std curves\n",
    "plt.fill_between(np.log10(alphas), mean_tr-std_tr, mean_tr+std_tr, alpha=0.2)\n",
    "plt.fill_between(np.log10(alphas), mean_te-std_te, mean_te+std_te, alpha=0.2)\n",
    "\n",
    "# Add marker for best score\n",
    "plt.scatter(np.log10(best_alpha), mean_te.min(), marker='x', c='red', zorder=10)\n",
    "\n",
    "plt.title('Validation curves with {} runs'.format(len(gs_results.groupby('run_idx'))))\n",
    "plt.xlabel('$log_{10}(alpha)$')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
